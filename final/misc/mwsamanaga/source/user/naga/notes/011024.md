Went back on that perfectly secure idea, interesting stuff, paper did a code release, found an implementation on github alrd adapting it n some other derivatives for easier stuff on logits. 

Had to modify the initialization a bit, since BOS and shit wacky, snippet for future ref (note 4 future me: You'll need to modify the marginal and stuff as well to work with this, also that "\n" in marginal's __init__ is... weird. I got rid of it):
```python
class LLM:
    def __init__(self, model_name: str = "gpt2"):
        """Initialize language model and tokenizer.

        Args:
            model_name: Name of the model from HuggingFace hub.
        """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name, device_map=self.device)
        self.vocab_size = self.model.config.vocab_size
        self.vocab = np.array(
            [self.tokenizer.decode([i]) for i in range(self.vocab_size)]
        )
        self.model.to(self.device)

    def conditional(self, text: str, temperature: float) -> torch.Tensor:
        """Compute conditional distribution given text."""
        with torch.no_grad():
            encoded_input = self.tokenizer(
                text,
                return_tensors="pt",
            ).to(self.device)
            outputs = self.model(encoded_input["input_ids"])
            logits = outputs[0][:, -1, :] / temperature
            return nn.Softmax(dim=-1)(logits)

    def top_k_conditional(self, text: str, temperature: float, k: int) -> np.ndarray:
        """Compute top-k conditional distribution given text."""
        conditional = self.conditional(text, temperature)
        conditional = conditional.to(dtype=torch.float32)
        kth = torch.topk(conditional, k).values.flatten()[-1]
        conditional[conditional < kth] = 0
        conditional /= conditional.sum()
        return conditional.cpu().numpy().reshape(-1)

    def reduced_ids(self, prompt: str, text: str, k: int) -> Optional[List[int]]:
        """Token IDs using indexing post top-k reduction."""
        prompt_tokens = self.tokenizer(prompt)["input_ids"]
        text_tokens = self.tokenizer(text)["input_ids"]
        if text_tokens[0] == self.tokenizer.bos_token_id:  # Skip BOS token
          text_tokens = text_tokens[1:]
        
        reduced_tokens = []

        for i, token in enumerate(text_tokens):
            inputs = torch.tensor([prompt_tokens + text_tokens[:i]]).to(self.device)
            outputs = self.model(inputs)[0][0, -1, :]
            top_k_idx = outputs >= torch.topk(outputs, k).values.flatten()[-1]
            if top_k_idx[token]:
                correct_idx = torch.arange(outputs.shape[-1], device=self.device)[top_k_idx] == token
                assert correct_idx.sum() == 1
                reduced_tokens.append(correct_idx.nonzero(as_tuple=True)[0].item())
            else:
                print(f"Failed at position {i}")
                print(f"Token: {self.tokenizer.decode([token])}")
                print(f"Top k tokens: {self.tokenizer.decode(torch.where(top_k_idx)[0].tolist())}")
                return None
        return reduced_tokens
```

Had to basically adapt the marginal for it. Worked like a charm lol. Fwiw problems are:
1. This assumes that the token will always be in k, and if broken, means no decoding for you
2. "perfectly secure" only goes as far as: A.) This has very low KL divergence; B.) If we encrypt prior to embedding via some simple XOR w/ a secret or smth, it's "perfectly" secure, since you'd need the key to get the thing from the stegotext.
3. Slow, not bad slow, but eh, slow.


